/**
 * ğŸš€ é©å‘½æ€§è¯·æ±‚å¤„ç†å™¨
 * âœ¨ å®Œå…¨é‡å†™ï¼Œæ— ç¡¬ç¼–ç é™åˆ¶ï¼
 * ğŸ¨ å®Œæ•´çš„å¤šæ¨¡æ€ã€å‡½æ•°è°ƒç”¨å’ŒåŠ¨æ€æ¨¡å‹æ”¯æŒ
 */

import * as http from 'http';
import * as vscode from 'vscode';

import { logger } from '../utils/Logger';
import { Converter } from '../utils/Converter';
import { Validator, ValidationError } from '../utils/Validator';
import { ModelDiscoveryService } from '../services/ModelDiscoveryService';
import { FunctionCallService } from '../services/FunctionCallService';

import {
    ModelCapabilities,
    DynamicModelCriteria,
    EnhancedMessage,
    EnhancedRequestContext,
    FunctionDefinition,
    ToolCall
} from '../types/ModelCapabilities';

import { ServerState } from '../types/VSCode';
import { 
    HTTP_STATUS, 
    CONTENT_TYPES, 
    SSE_HEADERS,
    ERROR_CODES,
    NOTIFICATIONS
} from '../constants/Config';

export class RequestHandler {
    private modelDiscovery: ModelDiscoveryService;
    private functionService: FunctionCallService;
    private requestMetrics: Map<string, { startTime: Date; model?: string }> = new Map();
    private isInitialized: boolean = false;
    
    constructor() {
        this.modelDiscovery = new ModelDiscoveryService();
        this.functionService = new FunctionCallService();
        
        // å¼‚æ­¥åˆå§‹åŒ–
        this.initialize();
    }
    
    /**
     * ğŸš€ åˆå§‹åŒ–å¤„ç†å™¨
     */
    private async initialize(): Promise<void> {
        try {
            logger.info('ğŸš€ Initializing Enhanced Request Handler...');
            
            // å‘ç°æ‰€æœ‰å¯ç”¨æ¨¡å‹
            await this.modelDiscovery.discoverAllModels();
            
            this.isInitialized = true;
            logger.info('âœ… Enhanced Request Handler initialized successfully!');
            
        } catch (error) {
            logger.error('âŒ Failed to initialize Enhanced Request Handler:', error as Error);
        }
    }
    
    /**
     * ğŸ¨ å¤„ç†èŠå¤©å®Œæˆï¼Œå…·å¤‡å®Œæ•´å¤šæ¨¡æ€æ”¯æŒ
     */
    public async handleChatCompletions(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string
    ): Promise<void> {
        const requestLogger = logger.createRequestLogger(requestId);
        const startTime = Date.now();
        
        try {
            // ç¡®ä¿æˆ‘ä»¬å·²åˆå§‹åŒ–
            if (!this.isInitialized) {
                await this.initialize();
            }
            
            requestLogger.info('ğŸš€ Processing enhanced chat completion request');
            
            // è¯»å–å¹¶è§£æè¯·æ±‚ä½“å¹¶éªŒè¯
            const body = await this.readRequestBody(req);
            
            // è§£æJSONå¹¶å¤„ç†é”™è¯¯
            let rawRequestData: any;
            try {
                rawRequestData = JSON.parse(body);
            } catch (parseError) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_REQUEST,
                    'Invalid JSON in request body',
                    ERROR_CODES.INVALID_REQUEST,
                    requestId
                );
                return;
            }
            
            // ä½¿ç”¨éªŒè¯å™¨éªŒè¯è¯·æ±‚
            let validatedRequest: any;
            try {
                validatedRequest = Validator.validateChatCompletionRequest(
                    rawRequestData,
                    this.modelDiscovery.getAllModels()
                );
            } catch (validationError) {
                if (validationError instanceof ValidationError) {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        validationError.message,
                        validationError.code,
                        requestId,
                        validationError.param
                    );
                } else {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_REQUEST,
                        'Request validation failed',
                        ERROR_CODES.INVALID_REQUEST,
                        requestId
                    );
                }
                return;
            }
            
            const requestData = validatedRequest;
            
            // æå–å¢å¼ºæ¶ˆæ¯å’Œè¯·æ±‚å‚æ•°
            const messages: EnhancedMessage[] = requestData.messages;
            const requestedModel = requestData.model || 'auto-select';
            const isStream = requestData.stream || false;
            const functions: FunctionDefinition[] = requestData.functions || [];
            const tools = requestData.tools || [];
            
            requestLogger.info('ğŸ“‹ Request analysis:', {
                model: requestedModel,
                stream: isStream,
                messageCount: messages.length,
                hasImages: messages.some(m => Array.isArray(m.content) && 
                    m.content.some(p => p.type === 'image_url')),
                hasFunctions: functions.length > 0 || tools.length > 0
            });
            
            // åˆ›å»ºå¢å¼ºä¸Šä¸‹æ–‡
            const context = Converter.createEnhancedContext(
                requestId,
                requestedModel,
                isStream,
                messages,
                undefined, // Will be set after model selection
                this.getClientIP(req),
                req.headers['user-agent']
            );
            
            // ğŸ¯ æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼ˆæ— ç¡¬ç¼–ç é™åˆ¶ï¼ï¼‰
            const selectionCriteria: DynamicModelCriteria = {
                preferredModels: requestedModel !== 'auto-select' ? [requestedModel] : undefined,
                requiredCapabilities: context.requiredCapabilities as any,
                requiresVision: context.hasImages,
                requiresTools: context.hasFunctions || functions.length > 0,
                minContextTokens: context.estimatedTokens,
                sortBy: 'capabilities'
            };
            
            const selectedModel = await this.modelDiscovery.selectOptimalModel(selectionCriteria);
            
            if (!selectedModel) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.SERVICE_UNAVAILABLE,
                    `No suitable model available for request requirements`,
                    ERROR_CODES.API_ERROR,
                    requestId
                );
                return;
            }
            
            // ç”¨æ‰€é€‰æ¨¡å‹æ›´æ–°ä¸Šä¸‹æ–‡
            context.selectedModel = selectedModel;
            
            requestLogger.info('âœ… Model selected:', {
                modelId: selectedModel.id,
                vendor: selectedModel.vendor,
                family: selectedModel.family,
                maxTokens: selectedModel.maxInputTokens,
                supportsVision: selectedModel.supportsVision,
                supportsTools: selectedModel.supportsTools
            });
            
            // æ£€æŸ¥ Copilot è®¿é—®æƒé™
            const hasAccess = await this.checkCopilotAccess();
            if (!hasAccess) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.UNAUTHORIZED,
                    NOTIFICATIONS.NO_COPILOT_ACCESS,
                    ERROR_CODES.AUTHENTICATION_ERROR,
                    requestId
                );
                return;
            }
            
            // éªŒè¯ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼ˆåŠ¨æ€ï¼ï¼‰
            if (context.estimatedTokens > selectedModel.maxInputTokens) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.BAD_REQUEST,
                    `Request exceeds model context limit (${context.estimatedTokens} > ${selectedModel.maxInputTokens} tokens)`,
                    ERROR_CODES.INVALID_REQUEST,
                    requestId
                );
                return;
            }
            
            // å°†æ¶ˆæ¯è½¬æ¢ä¸º VS Code æ ¼å¼
            const vsCodeMessages = await Converter.convertMessagesToVSCode(
                messages, 
                selectedModel
            );
            
            // å¦‚æœæ”¯æŒåˆ™å‡†å¤‡å·¥å…·
            let vsCodeTools: any[] = [];
            if (selectedModel.supportsTools && (functions.length > 0 || tools.length > 0)) {
                try {
                    vsCodeTools = this.functionService.convertFunctionsToTools(functions);
                    requestLogger.info(`ğŸ› ï¸ Prepared ${vsCodeTools.length} tools for execution`);
                } catch (error) {
                    requestLogger.warn('Failed to prepare tools:', { error: String(error) });
                }
            }
            
            // ğŸš€ å‘ VS CODE LM API å‘é€è¯·æ±‚
            try {
                requestLogger.info('ğŸ“¨ Sending request to VS Code LM API...');
                
                const requestOptions: any = {
                    tools: vsCodeTools.length > 0 ? vsCodeTools : undefined
                };
                
                const response = await selectedModel.vsCodeModel.sendRequest(
                    vsCodeMessages,
                    requestOptions,
                    new vscode.CancellationTokenSource().token
                );
                
                // ğŸŒŠ å¤„ç†æµå¼ä¸éæµå¼å“åº”
                if (isStream) {
                    await this.handleStreamingResponse(response, res, context, requestLogger);
                } else {
                    await this.handleNonStreamingResponse(response, res, context, requestLogger);
                }
                
            } catch (lmError) {
                requestLogger.error('âŒ VS Code LM API error:', lmError as Error);
                
                // ä½¿ç”¨å¢å¼ºé”™è¯¯æ˜ å°„å¤„ç†ç‰¹å®šçš„ LM API é”™è¯¯
                if (lmError instanceof vscode.LanguageModelError) {
                    this.handleLanguageModelError(lmError, res, requestId);
                } else {
                    this.sendErrorResponse(
                        res,
                        HTTP_STATUS.BAD_GATEWAY,
                        `Language model request failed: ${lmError}`,
                        ERROR_CODES.API_ERROR,
                        requestId
                    );
                }
            }
            
        } catch (error) {
            const duration = Date.now() - startTime;
            requestLogger.error(`âŒ Request failed after ${duration}ms:`, error as Error);
            
            if (!res.headersSent) {
                this.sendErrorResponse(
                    res,
                    HTTP_STATUS.INTERNAL_SERVER_ERROR,
                    'Enhanced request processing failed',
                    ERROR_CODES.API_ERROR,
                    requestId
                );
            }
        }
    }
    
    /**
     * ğŸ“‹ å¤„ç†å¢å¼ºæ¨¡å‹ç«¯ç‚¹
     */
    public async handleModels(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string
    ): Promise<void> {
        const requestLogger = logger.createRequestLogger(requestId);
        
        try {
            requestLogger.info('ğŸ“‹ Fetching all available models (no limitations!)...');
            
            // ç¡®ä¿æˆ‘ä»¬å·²åˆå§‹åŒ–
            if (!this.isInitialized) {
                await this.initialize();
            }
            
            // è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
            const allModels = this.modelDiscovery.getAllModels();
            
            requestLogger.info(`ğŸ“Š Found ${allModels.length} total models:`);
            
            // ä¸ºé€æ˜åº¦è®°å½•æ¨¡å‹èƒ½åŠ›
            allModels.forEach(model => {
                requestLogger.info(`  âœ¨ ${model.id}: tokens=${model.maxInputTokens}, vision=${model.supportsVision}, tools=${model.supportsTools}`);
            });
            
            const modelsResponse = Converter.createModelsResponse(allModels);
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(modelsResponse, null, 2));
            
            requestLogger.info(`âœ… Models response sent with ${modelsResponse.data.length} models`);
            
        } catch (error) {
            requestLogger.error('âŒ Error handling models request:', error as Error);
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Failed to retrieve models',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸ‘©â€âš•ï¸ å¢å¼ºå¥åº·æ£€æŸ¥
     */
    public async handleHealth(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string,
        serverState: ServerState
    ): Promise<void> {
        try {
            const modelPool = this.modelDiscovery.getModelPool();
            const healthResponse = Converter.createHealthResponse(serverState, modelPool);
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(healthResponse, null, 2));
            
        } catch (error) {
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Health check failed',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸ“‹ å¢å¼ºçŠ¶æ€ç«¯ç‚¹
     */
    public async handleStatus(
        req: http.IncomingMessage,
        res: http.ServerResponse,
        requestId: string,
        serverState: ServerState
    ): Promise<void> {
        try {
            const modelPool = this.modelDiscovery.getModelPool();
            const toolStats = this.functionService.getToolStats();
            
            const status = {
                server: serverState,
                models: {
                    total: modelPool.primary.length + modelPool.secondary.length + modelPool.fallback.length,
                    primary: modelPool.primary.length,
                    secondary: modelPool.secondary.length,
                    fallback: modelPool.fallback.length,
                    unhealthy: modelPool.unhealthy.length,
                    lastUpdated: modelPool.lastUpdated.toISOString(),
                    capabilities: {
                        vision: modelPool.primary.filter(m => m.supportsVision).length,
                        tools: modelPool.primary.filter(m => m.supportsTools).length,
                        multimodal: modelPool.primary.filter(m => m.supportsMultimodal).length
                    }
                },
                tools: {
                    available: Object.keys(toolStats).length,
                    stats: toolStats
                },
                features: {
                    dynamicModelDiscovery: true,
                    multimodalSupport: true,
                    functionCalling: true,
                    noHardcodedLimitations: true,
                    autoModelSelection: true,
                    loadBalancing: true
                },
                copilot: {
                    available: await this.checkCopilotAccess(),
                    models: await this.getModelCount()
                },
                timestamp: new Date().toISOString()
            };
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(status, null, 2));
            
        } catch (error) {
            this.sendErrorResponse(
                res,
                HTTP_STATUS.INTERNAL_SERVER_ERROR,
                'Status check failed',
                ERROR_CODES.API_ERROR,
                requestId
            );
        }
    }
    
    /**
     * ğŸŒŠ å¤„ç†å¢å¼ºæµå¼å“åº”
     */
    private async handleStreamingResponse(
        response: vscode.LanguageModelChatResponse,
        res: http.ServerResponse,
        context: EnhancedRequestContext,
        requestLogger: any
    ): Promise<void> {
        res.writeHead(HTTP_STATUS.OK, SSE_HEADERS);
        
        try {
            requestLogger.info('ğŸŒŠ Starting enhanced streaming response...');
            
            let chunkCount = 0;
            
            for await (const chunk of Converter.extractStreamContent(
                response, 
                context, 
                context.selectedModel!
            )) {
                res.write(chunk);
                chunkCount++;
            }
            
            requestLogger.info(`âœ… Enhanced streaming completed: ${chunkCount} chunks sent`);
            
        } catch (error) {
            requestLogger.error('âŒ Enhanced streaming error:', error);
            
            const errorEvent = Converter.createSSEEvent('error', {
                message: 'Enhanced stream processing error',
                type: ERROR_CODES.API_ERROR
            });
            res.write(errorEvent);
        } finally {
            res.end();
        }
    }
    
    /**
     * ğŸ“‹ å¤„ç†å¢å¼ºéæµå¼å“åº”
     */
    private async handleNonStreamingResponse(
        response: vscode.LanguageModelChatResponse,
        res: http.ServerResponse,
        context: EnhancedRequestContext,
        requestLogger: any
    ): Promise<void> {
        try {
            requestLogger.info('ğŸ“‹ Collecting enhanced full response...');
            
            const fullContent = await Converter.collectFullResponse(response);
            
            const completionResponse = Converter.createCompletionResponse(
                fullContent, 
                context,
                context.selectedModel!
            );
            
            res.writeHead(HTTP_STATUS.OK, { 'Content-Type': CONTENT_TYPES.JSON });
            res.end(JSON.stringify(completionResponse, null, 2));
            
            requestLogger.info('âœ… Enhanced response sent:', {
                contentLength: fullContent.length,
                tokens: completionResponse.usage.total_tokens,
                model: context.selectedModel!.id
            });
            
        } catch (error) {
            requestLogger.error('âŒ Error collecting enhanced response:', error as Error);
            throw error;
        }
    }
    
    /**
     * ğŸ”® æ£€æŸ¥ Copilot è®¿é—®æƒé™
     */
    private async checkCopilotAccess(): Promise<boolean> {
        try {
            const models = await vscode.lm.selectChatModels({ vendor: 'copilot' });
            return models.length > 0;
        } catch (error) {
            logger.warn('Copilot access check failed:', { error: String(error) });
            return false;
        }
    }
    
    /**
     * ğŸ“‹ è·å–æ¨¡å‹æ€»æ•°
     */
    private async getModelCount(): Promise<number> {
        try {
            const models = await vscode.lm.selectChatModels();
            return models.length;
        } catch (error) {
            return 0;
        }
    }
    
    /**
     * âŒ å¤„ç† VS Code è¯­è¨€æ¨¡å‹ç‰¹å®šé”™è¯¯
     */
    private handleLanguageModelError(
        error: vscode.LanguageModelError,
        res: http.ServerResponse,
        requestId: string
    ): void {
        let statusCode: number = HTTP_STATUS.INTERNAL_SERVER_ERROR;
        let errorCode: string = ERROR_CODES.API_ERROR;
        let message = error.message;
        
        // å¢å¼ºé”™è¯¯æ˜ å°„
        switch (error.code) {
            case 'NoPermissions':
                statusCode = HTTP_STATUS.FORBIDDEN;
                errorCode = ERROR_CODES.PERMISSION_ERROR;
                message = 'Permission denied for language model access';
                break;
            case 'Blocked':
                statusCode = HTTP_STATUS.FORBIDDEN;
                errorCode = ERROR_CODES.PERMISSION_ERROR;
                message = 'Request blocked by content filter';
                break;
            case 'NotFound':
                statusCode = HTTP_STATUS.NOT_FOUND;
                errorCode = ERROR_CODES.NOT_FOUND_ERROR;
                message = 'Language model not found';
                break;
            case 'ContextLengthExceeded':
                statusCode = HTTP_STATUS.BAD_REQUEST;
                errorCode = ERROR_CODES.INVALID_REQUEST;
                message = 'Request exceeds context length limit';
                break;
            default:
                statusCode = HTTP_STATUS.BAD_GATEWAY;
                errorCode = ERROR_CODES.API_ERROR;
                message = `Language model error: ${error.message}`;
        }
        
        this.sendErrorResponse(res, statusCode, message, errorCode, requestId);
    }
    
    /**
     * âŒ å‘é€å¢å¼ºé”™è¯¯å“åº”
     */
    private sendErrorResponse(
        res: http.ServerResponse,
        statusCode: number,
        message: string,
        type: string,
        requestId: string,
        param?: string
    ): void {
        if (res.headersSent) {
            return;
        }
        
        const errorResponse = Converter.createErrorResponse(message, type, undefined, param);
        
        res.writeHead(statusCode, { 'Content-Type': CONTENT_TYPES.JSON });
        res.end(JSON.stringify(errorResponse, null, 2));
        
        logger.error(`âŒ Enhanced error response: ${statusCode}`, new Error(message), { type, param }, requestId);
    }
    
    /**
     * ğŸ“‹ è¯»å–è¯·æ±‚ä½“
     */
    private async readRequestBody(req: http.IncomingMessage): Promise<string> {
        return new Promise((resolve, reject) => {
            let body = '';
            
            req.on('data', chunk => {
                body += chunk;
                
                // ä¸ºå¤šæ¨¡æ€å†…å®¹å¢åŠ é™åˆ¶
                if (body.length > 50 * 1024 * 1024) { // 50MB limit for images
                    reject(new Error('Request body too large'));
                    return;
                }
            });
            
            req.on('end', () => resolve(body));
            req.on('error', reject);
        });
    }
    
    /**
     * ğŸ“ è·å–å®¢æˆ·ç«¯IPåœ°å€
     */
    private getClientIP(req: http.IncomingMessage): string {
        return (req.headers['x-forwarded-for'] as string)?.split(',')[0] ||
               req.connection.remoteAddress ||
               '127.0.0.1';
    }
    
    /**
     * ğŸ§¹ æ¸…ç†èµ„æº
     */
    public dispose(): void {
        this.modelDiscovery.dispose();
        this.functionService.dispose();
        this.requestMetrics.clear();
    }
}
